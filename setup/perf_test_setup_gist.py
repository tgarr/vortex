#!/usr/bin/env python3

import numpy as np
import os
import pickle
import sys
import time
import json
from derecho.cascade.external_client import ServiceClientAPI


NUM_EMB_PER_OBJ = 200  # < 1MB/4KB = 250
EMBEDDING_DIM = 960
NUM_KEY_PER_MAP_OBJ = 50000 # takes around 1MB memory
FLOAT_POINT_SIZE = 32  # currently only support 32-bit float TODO: add support for 64-bit float

np.random.seed(1234)             # make reproducible
script_dir = os.path.dirname(os.path.abspath(__file__))

OBJECT_POOLS_LIST = "object_pools.list"

DOC_EMB_MAP_FILENAME = "doc_emb_map.pkl"
#DOC_LIST_FILENAME = "doc_list.pkl"
CENTROIDS_FILENAME = "centroids.pkl"
CLUSTER_FILE_PREFIX = "cluster_"


SUBGROUP_TYPES = {
        "VCSS": "VolatileCascadeStoreWithStringKey",
        "PCSS": "PersistentCascadeStoreWithStringKey",
        "TCSS": "TriggerCascadeNoStoreWithStringKey"
        }




def create_object_pool(capi, basepath):
    # create object pools
    print("Creating object pools ...")
    fpath = os.path.join(basepath,OBJECT_POOLS_LIST)
    with open(fpath,"r") as list_file:
        for line in list_file:
            fields = line.strip().split(" ")
            pool_path = fields[0]
            subgroup_type = fields[1]
            subgroup_index = int(fields[2])
            if len(fields) >= 4:
                affinity_set_regex = fields[3].strip()
                res = capi.create_object_pool(pool_path,SUBGROUP_TYPES[subgroup_type],
                                              subgroup_index,
                                              affinity_set_regex=affinity_set_regex)
                if res:
                    res.get_result()
                else:
                    print(f"Failed to create the object pool: {pool_path}")
                    exit(1)
            else:
                print(f"  {pool_path} {subgroup_type} {subgroup_index}")
                res = capi.create_object_pool(pool_path,
                                              SUBGROUP_TYPES[subgroup_type],
                                              subgroup_index)
                if res:
                    res.get_result()
                else:
                    print(f"Failed to create the object pool: {pool_path}")
                    exit(1)


def get_embeddings(basepath, filename, d=1024):
    '''
    Load the embeddings from a pickle file.
    '''
    fpath = os.path.join(basepath, filename)
    with open(fpath, 'rb') as file:
        data = pickle.load(file)
    if isinstance(data, list):
        data = np.array(data)
    if data.dtype == np.float64 and FLOAT_POINT_SIZE == 32:
        float32_data = data.astype(np.float32)
        data = float32_data
    assert(data.shape[1] == d)
    return data


def break_into_chunks(num_embeddings, chunk_size):
    chunk_idxs = []
    num_chunks = num_embeddings // chunk_size + 1 if num_embeddings % chunk_size != 0 else num_embeddings // chunk_size
    remaining_embs_num = num_embeddings
    for i in range(num_chunks):
        start_idx = i*chunk_size
        end_idx = i*chunk_size + min(chunk_size, remaining_embs_num)
        chunk_idxs.append((start_idx, end_idx))
        remaining_embs_num -= chunk_size
    return chunk_idxs

    
def put_initial_embeddings_docs(capi, basepath):
    # 0. put answer mapping
    DOC_EMB_MAP = pickle.load(open(os.path.join(basepath, DOC_EMB_MAP_FILENAME), "rb"))
    print("Initializing: putting doc_emb map to cascade server ...")
    for i, (cluster_id, table_dict) in enumerate(DOC_EMB_MAP.items()):
        # break the table into chunks
        chunk_idx = break_into_chunks(len(table_dict), NUM_KEY_PER_MAP_OBJ)
        table_key_list = list(table_dict.keys())
        for j, (start_idx, end_idx) in enumerate(chunk_idx):
            key = f"/rag/doc/emb_doc_map/cluster{cluster_id}/{j}"
            table_dict_chunk = {k: table_dict[k] for k in table_key_list[start_idx:end_idx]}
            table_json = json.dumps(table_dict_chunk)
            res = capi.put(key, table_json.encode('utf-8'))
            if res:
                res.get_result()
            else:
                print(f"Failed to put the doc table to key: {key}")
                exit(1)
        print(f"         Put cluster{cluster_id} doc_emb_map size: {len(table_dict)}")
    # 1. Put centroids'embeddings to cascade.
    centroids_embs = get_embeddings(basepath, CENTROIDS_FILENAME, EMBEDDING_DIM)
    centroids_chunk_idx = break_into_chunks(centroids_embs.shape[0], NUM_EMB_PER_OBJ)
    print(f"Initilizing: put {centroids_embs.shape[0]} centroids embeddings to cascade")
    for i, (start_idx, end_idx) in enumerate(centroids_chunk_idx):
        key = f"/rag/emb/centroids_obj/{i}"
        centroids_embs_chunk = centroids_embs[start_idx:end_idx]
        res = capi.put(key, centroids_embs_chunk.tobytes())
        if res:
            res.get_result()
        else:
            print(f"Failed to put the centroids embeddings to key: {key}")
            exit(1)
    print("Initializing: putting clusters' embeddings and docs to cascade server ...")

    #doc_list = pickle.load(open(os.path.join(basepath, DOC_LIST_FILENAME), "rb"))
    # 2. Put clusters' embeddings and docs to cascade.
    centroid_count = centroids_embs.shape[0]
    cluster_file_name_list = [f'{CLUSTER_FILE_PREFIX}{count}.pkl' for count in range(centroid_count)]
    for cluster_id, cluster_file_name in enumerate(cluster_file_name_list):
        cluster_embs = get_embeddings(basepath, cluster_file_name, EMBEDDING_DIM)
        num_embeddings = cluster_embs.shape[0]
        cluster_chunk_idx = break_into_chunks(num_embeddings, NUM_EMB_PER_OBJ)
        for i, (start_idx, end_idx) in enumerate(cluster_chunk_idx):
            key = f"/rag/emb/cluster{cluster_id}/{i}"
            cluster_embs_chunk = cluster_embs[start_idx:end_idx]
            res = capi.put(key, cluster_embs_chunk.tobytes())
            if res:
                res.get_result()
            else:
                print(f"Failed to put the cluster embeddings to key: {key}")
                exit(1)
        # Put the corresponding docs to cascade
        # for emb_idx in range(num_embeddings):
        #     doc_idx = DOC_EMB_MAP[cluster_id][emb_idx]
        #     doc_key = f"/rag/doc/{doc_idx}"
        #     doc = doc_list[doc_idx]
        #     res = capi.put(doc_key, doc.encode('utf-8'))
        #     if res:
        #         res.get_result()
        #     else:
        #         print(f"Failed to put the doc to key: {doc_key}")
        #         exit(1)
        print(f"         Put cluster{cluster_id}, num {num_embeddings} emb+doc, {len(cluster_chunk_idx)} objs to cascade")
    print(f"Initialized embeddings")


    


def main(argv):
    print("Connecting to Cascade service ...")
    # get the datadir from the command line
    if len(argv) < 2:
        print("Usage: python3 perf_test_setup.py <path_to_data_folder>")
        exit(1)
    data_dir = argv[1]
    capi = ServiceClientAPI()
    create_object_pool(capi, script_dir)
    put_initial_embeddings_docs(capi, data_dir)
    print("Done!")


if __name__ == "__main__":
    main(sys.argv)